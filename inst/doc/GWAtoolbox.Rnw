\documentclass[a4paper, 10pt]{article}
\usepackage{graphicx}
\usepackage{makeidx}
\usepackage{latexsym}
\usepackage[pdftex,colorlinks]{hyperref}
% \usepackage[utf8]{inputenc}

\sloppy

\title{GWAtoolbox \\An R package for the fast processing of data from Genome-Wide Association Studies}
\author{Christian Fuchsberger \and Daniel Taliun \and Cristian Pattaro}
\date{08-03-2013}

% \VignetteIndexEntry{GWAtoolbox}

\newcounter{example_cnt}

\newenvironment{example}{\refstepcounter{example_cnt} \begin{quotation} \noindent \textbf{Example \arabic{example_cnt}} \sf}{$\lhd$ \end{quotation}}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}

\makeindex

\begin{document}
\maketitle
\newpage

\tableofcontents
\newpage

\section{Introduction}

\textbf{\emph{GWAtoolbox}} is an \emph{R} package for processing data originated from Genome-Wide Association Studies (GWAS). 
GWAS have become increasingly popular in the last years, leading to the discovery of hundreds of common genetic variants affecting the risk of diseases (such as diabetes, hypertension, chronic kidney disease, etc.) or the level of quantitative biological parameters.

Results from GWAS typically consist of large files where, for each single nucleotide polymorphism (SNP), statistics related to the association between the SNP and the studied trait are stored. 
The number of SNPs which is currently being analyzed in most GWAS is in excess of 2.5 Million and is expected to increase rapidly. 
For each individual SNP, the minimal information stored consists of the SNP unique name, chromosomal position, genotype (reference and non-reference alleles), frequency of the reference allele, SNP effect size, its standard error, and p-value. Additional information such as minor allele frequency (MAF) and  imputation quality are often provided.
As a consequence, the typical dimension of GWAS result files is of >2.5 Million rows by >10 columns, for a total file size which is often larger than 300 Mbytes.

With the aim of detecting common or less common genetic variants with modest effects, it is now common practice to pool results from individual studies into meta-analysis efforts which not rarely involve dozens of studies. 
In these consortia initiatives, each individual study contributes from one to several files, either because multiple traits are being analyzed or because different analyses on the same trait are needed.
Consequently, data analysts working in these consortia have to deal with a massive amount of files which need to be quality controlled to avoid problems during the meta-analysis process. 
As a result of the quality control (QC) process, some files could be found to be corrupted or erroneous so that new data upload is needed from individual studies. 
In this way, the loop between the consortium and the individual study analyst originates multiple file checks, until a satisfactory data quality is achieved.

When working with such large datasets in R, simple operations such as uploading the GWAS files into the R working space, file management, and data plotting, can take a considerable amount of time, and a systematic QC of hundreds of GWAS files can be unfeasible or may require several weeks.

The \emph{GWAtoolbox} provides a set of instruments to simplify the data handling in the framework of meta-analyses of GWA data.
The function \emph{gwasqc()} is capable to process a high number of GWAS data files in a single run, and producing several QC reports and figures. Routines for the between-study comparison are also provided to check systematic difference between files. In addition, the package contains annotation and graphical tools to assist the result interpretation.

\section{Installation}
The \emph{GWAtoolbox} package can be downloaded from \url{http://www.eurac.edu/GWAtoolbox.html}.
It requires R version 2.15.0 or higher. The installation procedure depends on the host operating system and user privileges. In the following, detailed installation instructions for a wide range of settings are provided.

\subsection{Windows}

\emph{GWAtoolbox} for Windows is distributed in compiled binary format. Installation:

\begin{itemize}
\item[1. ] Download the latest version of the package: \emph{GWAtoolbox\_X.Y.Z.zip}.
\item[2. ] Start the R program.
\item[3a.] If you have administrator privileges, you can install the package to the main R library:
	\begin{itemize}
	\item[i.] Execute the command:
		\begin{quotation}
		\noindent \texttt{install.packages("path/to/GWAtoolbox\_X.Y.Z.zip", repos=NULL)}
		\end{quotation}
		where \texttt{path/to} is the directory where the package was downloaded.
	\item[ii.] Load the package in R with the command:
		\begin{quotation}
		\noindent \texttt{library(GWAtoolbox)}
		\end{quotation}
	\end{itemize}
\item[3b.] If you do NOT have administrator privileges:
	\begin{itemize}
	\item[i.] Execute the command:
		\begin{quotation}
		\noindent \texttt{install.packages("path/to/GWAtoolbox\_X.Y.Z.zip",\linebreak lib="path/to/install/directory",\linebreak repos=NULL)}
		\end{quotation}
		where \texttt{path/to} is the directory where the package was downloaded, and \texttt{path/to/install/directory} is the path with your install directory.
	\item[ii.] Load the package in R with command:
		\begin{quotation}
		\noindent \texttt{library(GWAtoolbox, lib.loc = "path/to/install/directory")}
		\end{quotation}
	\end{itemize}
\end{itemize}

\subsection{Unix}

\emph{GWAtoolbox} for Unix is distributed in source format and, therefore, it needs to be compiled on the user machine. 
This requires the C/C++ compilers to be available on the system.
If the requirement is fulfilled, the package can be installed as follows:
\begin{itemize}
\item[1. ] Download the latest package version \emph{GWAtoolbox\_X.Y.Z.tar.gz}.
\item[2a.] If you have administrator privileges, you can install packages to the main R library:
	\begin{itemize}
	\item[i.] In the Unix shell execute the command:
		\begin{quotation}
		\noindent \texttt{R CMD INSTALL path/to/GWAtoolbox\_X.Y.Z.tar.gz}
		\end{quotation}
		where \texttt{path/to} is the directory where the package was downloaded.
	\item[ii.] Start the R program and load the package with the command:
		\begin{quotation}
		\noindent \texttt{library(GWAtoolbox)}
		\end{quotation}
	\end{itemize}
\item[2b.] If you do NOT have administrator privileges, follow the following steps:
	\begin{itemize}
	\item[i.] In the Unix shell execute the single line command:
		\begin{quotation}
		\noindent \texttt{R CMD INSTALL path/to/GWAtoolbox\_X.Y.Z.tar.gz \linebreak -l path/to/install/directory}
		\end{quotation}
		where \texttt{path/to} is the directory where the package was downloaded, and \texttt{path/to/install/directory} is the path with your install directory.
	\item[ii.] Start the R program and load the package with the command:
		\begin{quotation}
		\noindent \texttt{library(GWAtoolbox, lib.loc="path/to/install/directory")}
		\end{quotation}
	\end{itemize}
\end{itemize}

\subsection{Mac OS X}

\emph{GWAtoolbox} for Mac OS X is distributed in compiled binary format. The following steps describe the installation procedure:
\begin{itemize}
\item[1. ] Download the latest package version \emph{GWAtoolbox\_X.Y.Z.tar.gz}.
\item[2a.] To install from the Mac OS X shell (\emph{Terminal}):
\begin{itemize}
	\item[i.] If you have administrator privileges, you can install packages to the main R library:
		\begin{itemize}
		\item[A.] In the Mac OS X shell execute the command:
			\begin{quotation}
			\noindent \texttt{R CMD INSTALL path/to/GWAtoolbox\_X.Y.Z.tar.gz}
			\end{quotation}
			where \texttt{path/to} is the directory where the package was downloaded.
		\item[B.] Start the R program and load the package with the command:
			\begin{quotation}
			\noindent \texttt{library(GWAtoolbox)}
			\end{quotation}
		\end{itemize}
	\item[ii.] If you do NOT have administrator privileges:
		\begin{itemize}
		\item[A.] In the Mac OS X shell execute the single line command:
			\begin{quotation}
			\noindent \texttt{R CMD INSTALL path/to/GWAtoolbox\_X.Y.Z.tar.gz \linebreak -l path/to/install/directory}
			\end{quotation}
			where \texttt{path/to} is the directory where the package was downloaded, and \texttt{path/to/install/directory} is the path with your install directory.
		\item[B.] Start the R program and load the package with the command:
			\begin{quotation}
			\noindent \texttt{library(GWAtoolbox, lib.loc="path/to/install/directory")}
			\end{quotation}
		\end{itemize}
\end{itemize}
\item[2b.] To install from R:
\begin{itemize}
	\item[i. ] Start the R program.
	\item[ii.] If you have administrator privileges, you can install packages to the main R library:
		\begin{itemize}
		\item[A.] Execute the command:
			\begin{quotation}
			\noindent \texttt{install.packages("path/to/GWAtoolbox\_X.Y.Z.tar.gz", repos=NULL)}
			\end{quotation}
			where \texttt{path/to} is the directory where the package was downloaded.
		\item[B.] Load the package in R with the command:
			\begin{quotation}
			\noindent \texttt{library(GWAtoolbox)}
			\end{quotation}
		\end{itemize}
	\item[iii.] If you do NOT have administrator privileges:
		\begin{itemize}
		\item[A.] Execute the command:
			\begin{quotation}
			\noindent \texttt{install.packages("path/to/GWAtoolbox\_X.Y.Z.tar.gz",\linebreak lib="path/to/install/directory",\linebreak repos=NULL)}
			\end{quotation}
			where \texttt{path/to} is the directory where the package was downloaded, and \texttt{path/to/install/directory} is the path with your install directory.
		\item[B.] Load the package in R with the command:
			\begin{quotation}
			\noindent \texttt{library(GWAtoolbox, lib.loc = "path/to/install/directory")}
			\end{quotation}
		\end{itemize}
	\end{itemize}
\end{itemize}

\section{The Quality Control Workflow}

A careful and thorough data QC should be performed before starting any meta-analysis of GWAS data, especially when many studies are involved.
In this framework, we identified three complementary aspects of a good QC analysis:
\begin{enumerate}
\item Formal checking: control that all files that are going to be submitted to the meta-analysis fulfill some formatting guidelines, including:
\begin{itemize}
\item consistency of column names with meta-analysis guidelines;
\item presence of the minimal required information; 
\item data are in a format that can be analyzed (numeric, character, factor); 
\item all SNP identification numbers are unique; 
\item alleles are coded in letters/numbers as expected; 
\item missing values are coded in a consistent way; 
\item the field separator is as expected;
\item strand information is present and unequivocal;
\item the number of chromosomes and chromosome coding are as expected;
\end{itemize}
\item Quality checking: evaluating the quality of data in each single file.
This includes:
\begin{itemize}
\item assessing the presence of unexpected values for some of the items required for the meta-analysis (e.g.: negative p-values or negative standard errors);
\item assessing p-value inflation and analyzing p-value distribution;
\item assessing the distribution of the main summary statistics, including the effect estimates, their standard errors, genotype imputation quality, etc.
\end{itemize}
\item Global checking: identification of any systematic bias that may affect the analysis. This step is aimed to uncover studies that are systematically different from the others. This may happen when, for instance, analysts of one study forgot to log-transform the phenotype or apply the wrong model to the data.
\end{enumerate}

Formal and quality checks of individual studies are performed in \emph{GWAtoolbox} using the \emph{gwasqc()} function.
\emph{gwasqc()} was built to address specific requirements, specifically:
\begin{enumerate}
\item it allows rapid file processing and reporting;
\item it eliminates routine user operations;
\item it allows multi-format reporting, including \emph{HTML}, \emph{CSV}, and text files.

\end{enumerate}

The complete QC workflow can be summarized in four basic steps (see Figure \ref{fig:workflow}):

\begin{enumerate}
\item collect the GWAS data files;
\item write an input script to process of all GWAS files with the \emph{gwasqc()} function;
\item run the QC using \emph{gwasqc()};
\item analyze the QC results to uncover errors or inconsistencies.
\end{enumerate}
\begin{figure}[!ht]
\centering
\includegraphics[type=png, ext=.png, read=.png, width=0.7\linewidth]{workflow}
\caption{The quality control workflow.}
\label{fig:workflow}
\end{figure}

In the next sections we cover each of the four steps and describe the requirements for the input files and the precise content of all output files.

\section{GWAS Data Files}
GWAS data are usually stored as delimited text files.
The first line of the file is the header row that describes the content of every column.
The field separator between columns can be any among \emph{whitespace}, \emph{tabulation}, \emph{comma}, or \emph{semicolon}. 
The field separator must be the same for every row in the file, including the header.

There is a minimum set of columns, that every GWAS data file should contain. 
In \emph{GWAtoolbox}, the following information is required for every file:
\begin{itemize}
\item Marker name
\item Chromosome number or name
\item Marker position
\item Coded and non-coded allele
\item Allele frequency for coded allele
\item Strand
\item Imputation label
\item Imputation quality
\item Effect estimate
\item Standard error of the effect estimate
\item P-value
\end{itemize}

More non-mandatory items can be included in the data file as, for example, the study sample size, the SNP call rate for genotyped SNPs, the p-value of the Hardy-Weinberg equilibrium test for genotyped SNPs, etc.


\section{The Input Script}
\emph{gwasqc()} can analyze several GWAS data files consecutively. 
Instructions are provided to \emph{gwasqc()} through a script in a text file.
The format of the script file resembles the METAL input file format\footnote{http://www.sph.umich.edu/csg/abecasis/metal/}.

In the input script, the user can list all GWAS file names to be analyzed and specify the format of each single GWAS file, including column names, field separator, etc. 
In the case when more GWAS files are in the same format, file specifications can be entered only once for all files.
Example \ref{ex:full_script} illustrates the content of a hypothetical input script file.

\begin{example} 
\label{ex:full_script}
\begin{verbatim}
# Description of input data columns
MARKER        SNPID
CHR           Chromosome
POSITION      Position
N             n_total
ALLELE        coded_allele noncoded_allele
STRAND        strand
EFFECT        beta
STDERR        se
PVALUE        pval
FREQLABEL     allele_freq_coded_allele
IMPUTED       imputed
IMP_QUALITY   oevar_imp

# High quality filters
HQ_SNP   0.01   0.3

# Plotting filters
MAF      0.01   0.05
IMP      0.3    0.6

# Prefix for output files
PREFIX   res_

# Input file with GWA data
PROCESS  input_file.txt
\end{verbatim}
\end{example}

\subsection{Listing the Input Data Files}
The names of the GWAS data files are specified in the input script with the command \textbf{PROCESS}\index{PROCESS}\footnote{\emph{GWAtoolbox} supports single line feed ('\textbackslash n') character or carriage return character ('\textbackslash r') followed by line feed character as the line terminators in the input files.}
If multiple files have to be checked, multiple \textbf{PROCESS}\index{PROCESS} lines must be specified.

\begin{example}
The input script contains the following two lines:
\begin{verbatim}
PROCESS   input_file_1.txt
PROCESS   /dir_1/dir_2/input_file_2.csv
\end{verbatim}
QC is applied first to \emph{input\_file\_1.txt} and then to \emph{input\_file\_2.csv}.
When files reside on different directories, the full path must be specified.
\end{example}

\subsection{Describing the Input Data Columns}
\subsubsection{Field Separator}
The field separator may be different for each GWAS data file. 
\emph{gwasqc()} automatically detects the field separator of each input file \emph{based on the first 10 rows}. 
The user has the possibility to specify the separator manually for each GWAS file using the command \textbf{SEPARATOR}\index{SEPARATOR}. 
Table \ref{table:separator_cmd} lists all supported separators.
\begin{table}[!ht]
\begin{center}
\begin{tabular}{|l|l|}
\hline
Argument & Separator \\
\hline
COMMA\index{COMMA} & \emph{comma}\\
TAB\index{TAB} & \emph{tabulation}\\
WHITESPACE\index{WHITESPACE} & \emph{whitespace}\\
SEMICOLON\index{SEMICOLON} & \emph{semicolon}\\
\hline
\end{tabular}
\caption{The list of arguments for the SEPARATOR command.}
\label{table:separator_cmd}
\end{center}
\end{table}

\begin{example}
In the following input script:
\begin{verbatim}
PROCESS      input_file_1.txt 
SEPARATOR    TAB              
PROCESS      input_file_2.csv 
PROCESS      input_file_3.txt 
\end{verbatim}
the field separator for the input file \emph{input\_file\_1.txt} is determined automatically by \emph{gwasqc()}; the separator for the input files \emph{input\_file\_2.csv} and \emph{input\_file\_3.txt} is set to tabulation by the user.
\end{example}

If the user manually specifies the wrong field separator, then the file(s) still will be processed with this separator. As a consequence, the mandatory columns will not be detected and the user will see it in the final report.\\

\subsubsection{Missing Values}
By default, \emph{gwasqc()} assumes that missing values are labeled as \emph{NA}. However, the label for missing value can be specified manually by the user with the command \textbf{MISSING}\index{MISSING}.

\begin{example}
In the following input script:
\begin{verbatim}
MISSING     -                
PROCESS     input_file_1.txt 
MISSING     NA               
PROCESS     input_file_2.csv 
\end{verbatim}
the \emph{hyphen} symbol is set as symbol for missing value for the file \emph{input\_file\_1.txt}. Afterwards, the coding for missing vaules is changed to \emph{NA} and is used to process \emph{input\_file\_2.csv}.
\end{example}

\subsubsection{Column Names}
\label{section:column_names}
In table \ref{table:default_columns} the complete list of default column names for the GWAS data files is reported. 
These names identify uniquely the items in the GWAS data file.
\begin{table}[!ht]
\begin{center}
\begin{tabular}{|l|p{8cm}|}
\hline
Default column name(s) & Description \\
\hline
MARKER\index{MARKER} & Marker name \\
CHR\index{CHR} & Chromosome number or name \\
POSITION\index{POSITION} & Marker position \\
ALLELE1\index{ALLELE1}, ALLELE2\index{ALLELE2} & Coded and non-coded alleles \\
FREQLABEL\index{FREQLABEL} & Allele frequency for the coded allele \\
STRAND\index{STRAND} & Strand \\
IMPUTED\index{IMPUTED} & Label value indicating if the marker was imputed (1) or genotyped (0) \\
IMP\_QUALITY\index{IMP\_QUALITY} & Imputation quality statistics; this can be different depending on the software used for imputation: MACH's \emph{Rsq}, IMPUTE's \emph{properinfo}, ...  \\
EFFECT\index{EFFECT} & Effect estimate \\
STDERR\index{STDERR} & Standard error of the effect estimate \\
PVALUE\index{PVALUE} & P-value \\
HWE\_PVAL\index{HWE\_PVAL} & Hardy-Weinberg equilibrium p-value \\
CALLRATE\index{CALLRATE} & Genotype callrate \\
N\index{N} & Sample size \\
USED\_FOR\_IMP\index{USED\_FOR\_IMP} & Label value indicating if a marker was used for imputation (1) or not (0) \\
AVPOSTPROB\index{AVPOSTPROB} & Average posterior probability for imputed marker allele dosage \\
\hline
\end{tabular}
\caption{The default column names.}
\label{table:default_columns}
\end{center}
\end{table}

Given that different names can be provided with the GWAS data files, \emph{gwasqc()} allows to redefine the default values for every input file in the input script. 
The redefinition command consists of the default column name followed by a new column name. To redefine the default column names for \emph{coded} and
\emph{non-coded} alleles, the command \textbf{ALLELE}\index{ALLELE} is followed by two new column names.

\begin{example}
Let's assume to have two input files, \emph{input\_file\_1.txt} and \emph{input\_file\_2.txt}. In \emph{input\_file\_1.txt}, the column names for the effect estimate and its standard error are \emph{beta} and \emph{SE}, respectively. In \emph{input\_file\_2.txt}, the column name for the effect estimate is the same as in \emph{input\_file\_1.txt}, but the column name for the standard error is \emph{STDERR}. 
The correct column redefinitions are as follows:
\begin{verbatim}
EFFECT     beta
STDERR     SE
PROCESS    input_file_1.txt
STDERR     STDERR
PROCESS    input_file_2.csv
\end{verbatim}
First, we redefine column names for the file \emph{input\_file\_1.txt}. Notice that 
the column \emph{beta} doesn't need to be redefined for file \emph{input\_file\_2.csv}. 
However, for this file we need to redefine the column \emph{STDERR}, returning it to its default name.
\end{example}

\begin{example}
Consider an input file \emph{input\_file\_1.txt} with the following names for ALLELE1 and ALLELE2: \emph{myRefAllele} and \emph{myNonRefAllele}.
The new column definition is applied as follows:
\begin{verbatim}
ALLELE    myRefAllele myNonRefAllele
PROCESS   input_file_1.txt
\end{verbatim}
\end{example}

\subsubsection{Case Sensitivity}
By default, \emph{gwasqc()} assumes that column names of GWAS input files are case insensitive. For example, the names \emph{STDERR}, \emph{StdErr}, and \emph{STDErr} are all perfectly equivalent.
This behaviour can be changed for every input file in the input script using the command \textbf{CASESENSITIVE}\index{CASESENSITIVE}. Table \ref{table:casesensitive_cmd} lists all possible arguments.
\begin{table}[!ht]
\begin{center}
\begin{tabular}{|l|l|}
\hline
Argument & Description \\
\hline
0 & Column names are case insensitive (default) \\
1 & Column names are case sensitive \\
\hline
\end{tabular}
\caption{The list of arguments for the CASESENSITIVE command.}
\label{table:casesensitive_cmd}
\end{center}
\end{table}

\begin{example}
Consider the following commands:
\begin{verbatim}
CASESENSITIVE   1
PROCESS         input_file_1.txt
CASESENSITIVE   0
PROCESS         input_file_2.csv 
\end{verbatim}
Column names of \emph{input\_file\_1.txt} are case sensitive and must correspond exactly to the default column names; column names of \emph{input\_file\_2.csv} are case insensitive.
\end{example}

\subsection{Specifying Data Filters}

\subsubsection{Implausible Values Filters}
Often, there is the necessity to identify implausible values for the statistics that will be included in the meta-analysis. 
Implausible values for the effect estimate, for its standard error, or the p-value are sometimes generated by the software used for the association testing.
For example, in the case of a disease outcome with a small number of cases or of a SNP with very small MAF, statistical packages can report inconsistent results due to statistical algorithms that fail to converge because of data sparseness. 
Other types of inconsistencies can originate from errors in the file management. \\

In these situations, it is important to identify the SNPs with inconsistent values, so that they can be removed before starting the meta-analysis.
\emph{gwasqc()} can identify these values by using appropriate threshold values. The number of SNPs affected by this kind of problems is reported. 
In addition, these SNPs are excluded from the calculation of the data quality summary statistics.\\
Implausible value filters are used by \emph{gwasqc()} to identify implausible values. Table \ref{table:implausible_values} lists the columns for which the filters can be applied and their default thresholds.
\begin{table}[!ht]
\begin{center}
\begin{tabular}{|l|l|}
\hline
Default column name & Default thresholds \\
\hline
STDERR\index{STDERR} & $[0, 100000]$ \\
IMP\_QUALITY\index{IMP\_QUALITY} & $(0, 1.5)$ \\
PVALUE\index{PVALUE} & $(0, 1)$ \\
FREQLABEL\index{FREQLABEL} & $(0, 1)$ \\
HWE\_PVAL\index{HWE\_PVAL} & $(0, 1)$ \\
CALLRATE\index{CALLRATE}& $(0, 1)$ \\
\hline
\end{tabular}
\caption{The default implausible value filters.}
\label{table:implausible_values}
\end{center}
\end{table}

The default thresholds can be redefined for every column in the input script. 
The new thresholds for a column can be specified after the redefinition of the column name (see Section \ref{section:column_names}).

\begin{example}
Let's assume that the file \emph{input\_file\_1.txt} has a standard error column called \emph{STDERR} and 
that the corresponding column in the input file \emph{input\_file\_2.csv} is called \emph{SE}. In addition, the imputation quality column is defined as \emph{oevar\_imp} in both files.
The user can redefine the column names while applying different plausibility filters:
\begin{verbatim}
STDERR        STDERR 0 80000
IMP_QUALITY   oevar_imp 0 1
PROCESS       input_file_1.txt
STDERR        SE 0 100000
PROCESS       input_file_2.csv
\end{verbatim}
The file \emph{input\_file\_1.txt} has new $[0, 80000]$ thresholds for the standard error and new $(0, 1)$ thresholds for the imputation quality. 
For the file \emph{input\_file\_2.csv} the thresholds of $[0, 100000]$ will be applied to the standard error column, while
for the imputation quality column the same filters as for the \emph{input\_file\_1.txt} will be applied.
\end{example}

\subsubsection{High Quality Filters}
\label{section:hq_data_filter}
In many cases, analysts want to restrict the analyses to SNPs with high imputation quality and with not too small MAF.
We call these SNPs '\emph{high quality SNPs}', that is SNPs for which results should be quite robust.
In the special case, when estimating the genomic inflation factor, \emph{lambda}, to check for the presence of cryptic relatedness or hidden population sub-structures, it may be important to remove SNPs that could artificially inflate the number of significant hits. 
Summary statistics are calculated after excluding SNPs with low quality (\emph{CSV} report files). 
Table \ref{table:hq_values} lists the default thresholds for allele frequency and  imputation quality.
\begin{table}[!ht]
\begin{center}
\begin{tabular}{|l|l|}
\hline
Default column name & Default thresholds \\
\hline
FREQLABEL\index{FREQLABEL} & $> 0.01$ \\
IMP\_QUALITY\index{IMP\_QUALITY} & $> 0.3$ \\
\hline
\end{tabular}
\caption{The default high quality imputation filters.}
\label{table:hq_values}
\end{center}
\end{table}

The default thresholds can be redefined with the command \textbf{HQ\_SNP}\index{HQ\_SNP} for every input file in the input script. 
The command is followed by two values: the first one is the threshold for the MAF and the one is the threshold for the imputation quality.

\begin{example}
If we want to define 'high quality SNPs' those with MAF $> 0.03$ and imputation quality $> 0.4$, we would add the following lines to the input script:
\begin{verbatim}
HQ_SNP     0.03 0.4
PROCESS    input_file_1.txt
\end{verbatim}
\end{example}

\subsubsection{Plotting Filters}
\label{section:plotting_filter}
The plotting filters are used to select meaningful data to be displayed in the various summary plots. Each filter allows two threshold levels: each of them is applied dependently on the plot type and column. Figure \ref{fig:plots} (see Section \ref{section:number_and_content_of_plots}) shows what data and filters are used when producing plots. Table \ref{table:maf_imp_values} lists the default threshold values.
\begin{table}[!ht]
\begin{center}
\begin{tabular}{|l|p{3cm}|p{3cm}|}
\hline
Default column name & Default 1st level thresholds & Default 2nd level thresholds\\
\hline
FREQLABEL\index{FREQLABEL} & $> 0.01$ & $> 0.05$ \\
IMP\_QUALITY\index{IMP\_QUALITY} & $> 0.3$ & $> 0.6$ \\
\hline
\end{tabular}
\caption{The default plotting filter.}
\label{table:maf_imp_values}
\end{center}
\end{table}

The default threshold values for MAF and imputation quality can be redefined accordingly with the commands \textbf{MAF}\index{MAF} and \textbf{IMP}\index{IMP} for the every input file in the input script.

\begin{example}
Assume the input script contains the following commands:
\begin{verbatim}
MAF        0.02 0.03
IMP        0.3 0.5
PROCESS    input_file_1.txt
\end{verbatim}
Here, new SNP quality thresholds are set for plotting results from \emph{input\_file\_1.txt}. For the first level thresholds, we have selected MAF $> 0.02$ and the imputation quality $> 0.3$, for the second level threshold, MAF $> 0.03$ and imputation quality $> 0.5$.
\end{example}

\subsection{Controlling The Output}

\subsubsection{Output File Names}
Output file names are defined based on input file names, with the addition of a specified prefix (all types of output files will share the same prefix). The prefix can be specified once for all input files, or for every single input file or groups of input files explicitly using the command \textbf{PREFIX}\index{PREFIX}.

\begin{example}
Consider the following input script:
\begin{verbatim}
PREFIX       res_
PROCESS      input_file_1.txt
PROCESS      input_file_2.csv

PREFIX       result_
PROCESS      input_file_3.tab
\end{verbatim}
All result output files corresponding to the input files \emph{input\_file\_1.txt} and \emph{input\_file\_2.csv} will be prefixed with \emph{res\_}; result output files corresponding to the input file \emph{input\_file\_3.tab} will be prefixed with \emph{result\_}.
\end{example}

\subsubsection{Verbosity Level For Graphical Output}
The user can control the number of output pictures with the command \textbf{VERBOSITY}\index{VERBOSITY} (see Table \ref{table:verbosity_cmd} for the available options). 
\begin{table}[!ht]
\begin{center}
\begin{tabular}{|l|p{8cm}|}
\hline
Argument & Description \\
\hline
1 & Lowest verbosity level (default). \\
2 & Highest verbosity level. \\
\hline
\end{tabular}
\caption{The list of arguments for the VERBOSITY command.}
\label{table:verbosity_cmd}
\end{center}
\end{table}

\begin{example}
The input script contains the following commands:
\begin{verbatim}
VERBOSITY       2
PROCESS         input_file_1.txt
VERBOSITY       1
PROCESS         input_file_2.csv 
\end{verbatim}
The file \emph{input\_file\_1.txt} is processed with the highest verbosity level and therefore all figures are generated; \emph{input\_file\_2.csv} is processed with the lowest verbosity level and less output figures are generated.
\end{example}

\subsubsection{Number And Content Of Plots}
\label{section:number_and_content_of_plots}
Number and content of the output plots depend on the setting of the plotting filters (see Section \ref{section:plotting_filter}) and on the available information in the input files. Figure \ref{fig:plots} shows all the dependencies. If some dependency is not satisfied because of missing data or filter setting, then the corresponding plot is not produced or may be truncated at different levels.
\begin{figure}[!ht]
\centering
\includegraphics[type=png, ext=.png, read=.png, width=0.5\linewidth]{plots}
\caption{The dependencies of graphical outputs on columns and filters.}
\label{fig:plots}
\end{figure}

When multiple files are processed, boxplots from the distributions of the effect estimates are displayed in a single graph for across-study comparison.  It is possible to specify the width of each box based on one of the other available information (typically the sample size).
As an argument, \textbf{BOXPLOTWIDTH}\index{BOXPLOTWIDTH} requires one of the default column names.
If \textbf{BOXPLOTWIDTH}\index{BOXPLOTWIDTH} is not specified all boxplots have the same width.

It is also possible to define labels for each input file, to be used in the plots instead of the full file names, which could be too long and, therefore, clutter the plots.

\begin{example}
Let \emph{n\_total} be the column name which identifies the sample size in the input file \emph{input\_file\_1.txt}, and \emph{samplesize} the corresponding name in \emph{input\_file\_2.csv}.
Consider the following input script:
\begin{verbatim}
N              n_total
PROCESS        input_file_1.txt	first
N              samplesize
PROCESS        /dir_1/dir_2/input_file_2.csv	second
BOXPLOTWIDTH   N
\end{verbatim}
The width of the first boxplot for the input file \emph{input\_file\_1.txt} depends on the \emph{n\_total} column, while the width of the second boxplot for the input file \emph{input\_file\_2.csv} depends on the \emph{samplesize} column.
The labels "first" and "second" will be used to label the two studies in the plots.
\end{example}

\section{The Output Files}
The typical output of the \emph{gwasqc()} function consists of the following files:
\begin{enumerate}
\item \textbf{Graphical files} (\emph{PNG} file extension) include \textbf{histograms} and \textbf{boxplots} of the distribution of the main statistics from each GWAS file: effect estimates, imputation quality index, sample size, p-value, allele frequency; 
\textbf{QQ-plots} of the p-value distribution are also provided to investigate the presence of study-design bias.
See Figure \ref{fig:plots} for an exhaustive list of available plots.
\item \textbf{Textual report} (\emph{TXT} file extension) contains information on the GWAS file-format quality and statistics summarizing the distribution of all data present in the GWAS files (effect estimates, p-values, etc.).
The statistics provided in this report can be compared with the graphical output described above.
\item \textbf{Comma separated report} (\emph{CSV} file extension) contains summary statistics for the high quality SNPs, as they have been defined by the user when setting the parameter \textbf{HQ\_SNP}\index{HQ\_SNP} in the QC script (see Section \ref{section:hq_data_filter}).
The tabular format of this file is intended to be useful for users who wants to perform additional analyses and compare results from different GWAS files without having to manage the large original GWAS files.
Figure \ref{fig:csv_report_file} shows an example of such file.
\begin{figure}[!h]
\begin{center}
\tiny
\begin{tabular}{|l|p{1.8cm}|p{1.8cm}|p{1.8cm}|p{1.8cm}|p{1.8cm}|}
\hline
& N & EFFECT & STDERR & PVALUE & MAF \\
\hline
N & 2543887 & 2542617 & 2542422 & 2542617 & 2543887 \\
\hline
N\_HQ & 2448544 & 2448544 & 2448544 & 2448544 & 2448544 \\
\hline
N\_NAs & 0 & 1270 & 1270 & 1270 & 0 \\
\hline
Mean & 516 & -0.00137 & 0.295076 & 0.500823 & 0.230416 \\
\hline
StdDev & 0 & 0.333379 & 0.158119 & 0.28873 & 0.143312 \\
\hline
Min & 516 & -8.76038 & 0.17353 & 7.34E-08 & 0.010001 \\
\hline
Max & 516 & 6.12165 & 2.53341 & 1 & 0.5 \\
\hline
Median & 516 & -0.00149 & 0.233707 & 0.502629 & 0.217008 \\
\hline
Skewness & nan & -0.06933 & 2.886758 & -0.00935 & 0.214034 \\
\hline
Kurtosis & nan & 9.109873 & 11.14145 & -1.20085 & -1.18454 \\
\hline
\end{tabular}

\vspace{0.2cm}

\begin{tabular}{|l|p{1.8cm}|p{1.8cm}|p{1.8cm}|p{1.8cm}|p{1.8cm}|}
\hline
& IMPUTED & IMP\_QUALITY & CALLRATE & HWE\_PVAL & STD\_EFFECT\_0.5 \\
\hline
N & 2543887 & 2543887 & 2543887 & NA & NA \\
\hline
N\_HQ & 2448544 & 2448544 & 2448544 & NA & NA \\
\hline
N\_NAs & 0 & 0 & 0 & NA & NA \\
\hline
Mean & 1 & 0.958868 & 1 & NA & NA \\
\hline
StdDev & 0 & 0.097734 & 0 & NA & NA \\
\hline
Min & 1 & 0.300006 & 1 & NA & NA \\
\hline
Max & 1 & 1.0467 & 1 & NA & NA \\
\hline
Median & 1 & 0.994577 & 1 & NA & NA \\
\hline
Skewness & nan & -3.90226 & nan & NA & -0.00587 \\
\hline
Kurtosis & nan & 16.76683 & nan & NA & 3.297394 \\
\hline
\end{tabular}

\vspace{0.2cm}

\begin{tabular}{|l|p{1.8cm}|p{1.8cm}|p{1.8cm}|p{1.8cm}|}
\hline
& STD\_EFFECT\_0.75 & STD\_EFFECT\_0.95 & STD\_EFFECT\_0.99 & STD\_EFFECT\_1 \\
\hline
N & NA & NA & NA & NA \\
\hline
N\_HQ & NA & NA & NA & NA \\
\hline
N\_NAs & NA & NA & NA & NA \\
\hline
Mean & NA & NA & NA & NA \\
\hline
StdDev & NA & NA & NA & NA \\
\hline
Min & NA & NA & NA & NA \\
\hline
Max & NA & NA & NA & NA \\
\hline
Median & NA & NA & NA & NA \\
\hline
Skewness & -0.01036 & 0.022903 & 0.040024 & -0.07186 \\
\hline
Kurtosis & 3.822136 & 4.712606 & 5.861049 & 9.066722 \\
\hline
\end{tabular}
\caption{The comma separated report file.}
\label{fig:csv_report_file}
\end{center}
\end{figure}
For each of the variables listed by columns, summary statistics are reported by row.
Specifically:
\begin{itemize}
\item \textbf{N} -- number of SNPs with available information (i.e. non-missing values).
\item \textbf{N\_HQ} -- number of high quality SNPs with information available (i.e. non-missing values).
\item \textbf{N\_NAs} -- number of SNPs with missing values for the specific field of interest.
\item \textbf{Mean}, \textbf{StdDev}, \textbf{Min}, \textbf{Max}, \textbf{Median}, \textbf{Skewness}, and \textbf{Kurtosis} are referred to the distribution of the specific field.
\end{itemize}
Columns include:
\begin{itemize}
\item \textbf{N} -- study sample size.
\item \textbf{EFFECT}, \textbf{STDERR}, and \textbf{PVALUE} -- summaries of the SNP-phenotype associations.
\item \textbf{MAF}, \textbf{IMPUTED}, \textbf{IMP\_QUALITY}, \textbf{CALLRATE}, and \textbf{HWE\_PVAL} -- summaries of the genotype distribution and quality.
\item \textbf{STD\_EFFECT\_0.5}, \textbf{STD\_EFFECT\_0.75}, \textbf{STD\_EFFECT\_0.95}, \textbf{STD\_EFFECT\_0.99}, \textbf{STD\_EFFECT\_1} -- these columns are only of interest for the reported skewness and kurtosis of the standardized effect estimates (beta/SE).
The numbers 0.5, 0.75, 0.95, 0.99, and 1 are referred to the percentage of SNPs with the highest p-values chosen to study the effect size distribution (see Section \ref{section:comparing_skewness_and_kurtosis_of_effect_size_distribution} for additional details).
\end{itemize}
\item \textbf{An HTML report} (\emph{main.html} file) combines both textual and graphical output, allowing the user to easily surfing across the results and across the studies included in the QC analysis.
In operating systems allowing graphical interfaces, the \emph{HTML} document should be the first file to be opened to investigate the results.
\end{enumerate}

\section{Example}
This is an embedded R code example. All input files for this example are located in the subdirectory \emph{doc} of the installed \emph{GWAtoolbox} package.

Consider the following five GWAS data files: \emph{gwa\_data\_example\_1.txt}, \emph{gwa\_data\_example\_2.tbl}, \emph{gwa\_data\_example\_3.csv}, \emph{gwa\_data\_example\_4.txt} and \emph{gwa\_data\_example\_5.csv}. 
The first file contains 16 whitespace-separated columns:
<<>>=
t <- read.table("gwa_data_example_1.txt", header = T, nrow = 1, sep = " ")
colnames(t)
@
The second file contains 16 tab-separated columns:
<<>>=
t <- read.table("gwa_data_example_2.tbl", header = T, nrow = 1, sep = "\t")
colnames(t)
@
Analogously, we can preview the headers of the other three files.
To perform the QC of these files we prepare a simple input script: \emph{GWAS\_script.txt}. 
Below are listed the commands which were inlcuded in the script:
<<>>=
cat(readLines("GWASQC_script.txt"), sep = "\n")
@
When the input script is ready, load the \emph{GWAtoolbox} library and call the \emph{gwasqc()} function as follows: 
<<results=hide>>=
library(GWAtoolbox)
gwasqc("GWASQC_script.txt")
@

\section{Parallel Processing}
\emph{gwasqc()} processes GWAS data files sequentially, one after another.
It makes possible to handle very large files on desktop machines with relatively small amounts of available main memory.
\emph{pgwasqc()} function is analogous to \emph{gwasqc()} and processes GWAS data files in parallel.
The processing of several files is distributed among available CPUs at an additional cost of main memory.
The computational time of \emph{pgwasqc()} highly depends on the configuration of file storage and synchronization of I/O operations.

\section{Between-study comparisons}
\subsection{Comparing skewness and kurtosis of effect size distribution}
\label{section:comparing_skewness_and_kurtosis_of_effect_size_distribution}
Association of genetic markers with continuous or binary phenotypes is generally assessed by the use of linear models, where an effect estimate and its standard error are used to summarize the evidence of the association.
The effect estimate is usually represented by the beta coefficient of  the linear regression model.
For binary outcome, this correspond to the log(odds ratio) obtained from logistic regression models.
Let's define with $\theta$ our parameter of interest, and with $SE(\theta)$ its standard error.
For large sample sizes, under the null hypothesis of no association, the distribution of $\theta/SE(\theta) \sim N(0,1)$.

In checking the quality of GWA results, we are interested in assessing potential errors arisen during the analytical process or during the file management process.
These errors could origin $\theta/SE(\theta)$ distributions that are systematically biased towards positive (or negative) values, or that are over/under-dispersed.
The \emph{kurtosis} and the \emph{skewness} indices are the natural candidates to perform this kind of assessment.
Convenient graphical display based on these two indices enables the contemporary plot and comparison of all the studies involved in the meta-analysis, with consequent identification of studies that are systematically different from each other.

Under the forms proposed by Cramer \cite{Cramer46}, the kurtosis and the skewness indices can be defined as
$ku=\frac{\frac{1}{n}\sum_{i=1}^{n}(x_i - \bar{x})^4}{\big(\frac{1}{n}\sum_{i=1}^{n}(x_i - \bar{x})^2\big)^2}-3$
and
$sk=\frac{\frac{1}{n}\sum_{i=1}^{n}(x_i - \bar{x})^3}{\big(\frac{1}{n}\sum_{i=1}^{n}(x_i - \bar{x})^2\big)^\frac{3}{2}}$.
The skewness index assesses the symmetry of a distribution around its central value, and the kurtosis index assesses the dispersion of the distribution around its central value.
If $\theta/SE(\theta) \sim N(0,1)$, then for large sample sizes, $sk_\theta=sk\big(\theta/SE(\theta)\big) \rightarrow 0$ and $ku_\theta=ku\big(\theta/SE(\theta)\big) \rightarrow 0$ (Fisher \cite{Fisher30}; Joanes and Gill \cite{Joanes98}).

In a GWAS setting, we can assume that the 50\% of SNPs with largest p-value are not associated with the phenotype of interest and so, they can be used to represent the null situation.
Notice that the 50\% of SNPs with worst p-values correspond to the concept of the genomic control inflation factor, which is estimated based on the median chi-square distribution from the p-values.

In real world applications, distribution of $\theta/SE(\theta)$ for 50\% worst SNPs is not normally distributed, because SNPs are not independent each other.
This situation is more pronounced in presence of genotype imputation, which causes an excess of effect estimetes that are close to 0.
For this reason, it is not realistic to expect that $ku_\theta \rightarrow 0$: the distribution is leptokuric, and so $ku_\theta > 0$.
However, as long as the studies involved in the meta-analysis used similar imputation reference platforms, $ku_\theta$ should be similar for all studies.
For what concerns the skewness, there is no good reason why $sk_\theta$ of the 50\% worst SNPs shouldn't approximate $0$.

Then, for given $K$ studies, we can estimate $ku_{\theta,k}$ and $sk_{\theta,k}$ for $k = 1, \dots, K$, and we can plot the two vectors $sk_{\theta,[1,\dots,K]}$ vs. $ku_{\theta,[1,\dots,K]}$ in a Cartesian diagram, with every point representing a different study.
We expect all studies to cluster around the same point at $sk=0$, with similar kurtosis values.
Studies that show strong departures from the main cluster could be submitted to detailed investigation in order to detect the reason of such discrepancy.
In general, departures along the $sk$-axis are more serious than departures on the $ku$-axis, because the first ones will introduce systematic bias in the meta-analysis with one or a few studies that are systematically different from the others, with effect estimates that are more often in one direction.

This diagnostic plot is shown in Figure \ref{fig:kusk_check} where the set of SNPs with 50\% largest p-values is compared with other sets of SNPs with the largest 75\%, 95\%, and 100\% (i.e. all SNPs are considered) p-values, respectively.
Highlighted are points (studies) that are largely difference from the ones in the cluster.
In the last scenario, all SNPs are included and the bias is given by the SNPs that are truly associated with the phenotype. 
Outlier studies, that have been identified in the 50\% scatterplot, are colored in red also in the other situations for comparison.
\begin{figure}[!h]
\centering
\includegraphics[type=png, ext=.png, read=.png, width=\textwidth]{kusk_check}
\caption{The skewness and kurtosis plot.}
\label{fig:kusk_check}
\end{figure} 

The \emph{GWAtoolbox} allows automatic comparison of skewness and kurtosis of effect size distribution between GWA studies.
The \emph{gwasqc()} function estimates the skewness and kurtosis statistics during the QC workflow and includes them into the \emph{CSV} reports. 
Then, the auxiliary \emph{kusk\_check()} function can be used to export this information to a \emph{R} data frame and to produce diagnostic plots.
As input, it requires the same script used for \emph{gwasqc()} and assumes that all the \emph{CSV} reports are located in the current working directory.
An optional list consisting of any of integer number among 50, 75, 95, 99, and 100, can be specified: numbers correspond to the percentage of SNPs to be considered as representing the null distribution.

\begin{example}
We report the commands to obtain the scatterplot shown in Figure \ref{fig:kusk_check}, when the 50\% SNPs with largest p-values is considered.  
<<results=hide>>=
W <- kusk_check("GWASQC_script.txt", worst = c(50), plot = TRUE)
points(W$sk50[W$ku50 > 5], W$ku50[W$ku50 > 5], pch = 22, bg = 2, cex = 2)
text(W$sk50[W$ku50 > 5], W$ku50[W$ku50 > 5], labels = W$study[W$ku50 > 5], cex = 1, pos = 4)
@
\end{example} 

Currently, no automatic method to identify outlier studies is implemented and that the user needs to define his/her own criteria for the outlier identificaton.\\

\subsection{Precision of the effect estimates by sample size}
\label{section:effect_estimates_precision_by_sample_size}
A different graphical test, that allows the comparison of studies against each other, is based on the assessment of the distribution of estimates' precision vs. study sample size.
In general, the average $SE(\theta)$ is expected to be inversely proportional to the study sample size.
The auxiliary \emph{dispersion\_check()} function plots a scatterplot of the $mean(SE(\theta))$ vs. the median sample size of all studies, as depicted in the example Figure \ref{fig:dispersion_check}.
Over-dispersion is defined as the presence of larger SEs than expected given the study sample size and under-dispersion is meant to be the opposite phenomenon.
For example, a study with unmodeled relatedness or population stratification may present SEs that are smaller than another study of similar sample size where these issues were accounted for properly. 
\begin{figure}[!ht]
\centering
\includegraphics[type=png, ext=.png, read=.png, width=\textwidth]{dispersion}
\caption{Schematic representation of the dispersion plot and its interpretation.}
\label{fig:dispersion_check}
\end{figure} 

The \emph{dispersion\_check()} function uses the \emph{CSV} reports generated by \emph{gwasqc()}.
As input it requires the same script as \emph{gwasqc()} and assumes that all the \emph{CSV} reports files are located in the current working directory.
If the study sample size is missing from GWAS files, then the function has an optional parameter allowing to specify a vector with all study sample sizes.
The function returns an \emph{R} data frame with the information extracted from the \emph{CSV} reports and produces the diagnostic plot.

\begin{example}
<<results=hide>>=
Z <- dispersion_check("GWASQC_script.txt", plot=TRUE)
@
<<>>=
Z
@
<<results=hide>>=
text(Z$median_n, Z$mean_se, labels=Z$study, pos=c(2,4,2,1,2))
@
<<results=hide>>=
Z <- dispersion_check("GWASQC_script.txt", sample_sizes=c(1200, 200, 1000, 500, 700), plot=TRUE)
@
<<>>=
Z
@
<<results=hide>>=
text(Z$median_n, Z$mean_se, labels=Z$study, pos=c(2,4,2,1,2))
@
\end{example} 

\section{Additional Tools}

\subsection{GWAS Data Files Formatting}
The format of GWAS data files may differ between studies.
Inconsistencies in column names, column separators and column ordering are the most common.
For the automated data analysis it is more convenient to have a uniform format across all the GWAS files of interest.
The \emph{gwasformat()} function and its parallel version \emph{pgwasformat()} allow to quickly transform GWAS data files into a uniform format.
In addition to the possiblity to rename and re-order columns, these functions calculate effective sample size, inflation factor and apply genomic control of P-values and standard errors.
The column separator is replaced with tabulation in all processed files.

\subsubsection{Input Script}
The formatting instructions are provided to \emph{gwasformat()} through the input script that is identical to the script required by \emph{gwasqc()}.
Additional commands are supported allowing to rename and re-order columns, enable/disable genomic control.
Example \ref{ex:full_format_script} illustrates the content of a hypothetical input script file.

\begin{example} 
\label{ex:full_format_script}
\begin{verbatim}
# Rename columns
RENAME  SNPID     rsId
RENAME  chr       chrom
RENAME  position  bp
RENAME  SE        StdErr
RENAME  pval      p-value
RENAME  p         p-value
RENAME  pvalue    p-value

# Description (meaning) of the columns
ALLELE        coded_all noncoded_all
CALLRATE      callrate
CHR           chrom
EFFECT        beta
FREQLABEL     AF_coded_all 
HWE_PVAL      HWE_pval
IMPUTED       imputed
IMP_QUALITY   oevar_imp
MARKER        rsId
N             n_total
POSITION      bp
PVALUE        p-value
STRAND        strand_genome
STDERR        StdErr
USED_FOR_IMP  used_for_imp
AVPOSTPROB    avpostprob

# Enable computation of inflation factor and genomic control
GC	ON

# Set filters on minor allele frequency and imputation quality
HQ_SNP	0.05 0.4

# Enable column re-ordering and specify the order explicitely
ORDER	ON	chrom bp rsId strand_genome coded_all noncoded_all

# Prefix for output files
PREFIX	pgwasformat_

# Input file with GWA data
PROCESS  input_file.txt
\end{verbatim}
\end{example}

\subsubsection{Renaming Columns}
The new column names are specified in the input script with the command \textbf{RENAME}\index{RENAME}.
The command is followed by two words: the first one corresponds to the column name in the original GWAS file, and the second one corresponds to the new column name in the resulting formatted GWAS file.
The column names must not contain tabulation or whitespace characters.
If column was renamed, then only the new column name must be used in the rest of the input script commands.

\begin{example}
Let's assume to have three input files: \emph{input\_file\_1.txt}, \emph{input\_file\_2.csv} and \emph{input\_file\_3.txt}.
The files have column named \emph{marker}.
In file \emph{input\_file\_1.txt} the column should be renamed to \emph{SNPID}.
While in files \emph{input\_file\_2.csv} and \emph{input\_file\_3.txt} it should be renamed to \emph{rsId}.
The correct column renaming is as follows:
\begin{verbatim}
RENAME   marker SNPID
PROCESS  input_file_1.txt
RENAME   marker rsId
PROCESS  input_file_2.csv
PROCESS  input_file_3.txt
\end{verbatim}
\end{example}

\subsubsection{Ordering Columns}
By default, \emph{gwasformat()} doesn't change column ordering followed in the original GWAS file.
This behaviour can be modified for every input file in the input script using the command \textbf{ORDER}\index{ORDER}.
Table \ref{table:order_cmd} lists all possible arguments.
\begin{table}[!ht]
\begin{center}
\begin{tabular}{|p{4cm}|p{7.5cm}|}
\hline
Argument & Description \\
\hline
OFF & The original column ordering is preserved in the resulting formatted GWAS file (default) \\
ON & Columns are re-ordered following the alphabetical ordering \\
ON column\_1 column\_2 ... & Columns are re-ordered following the specified order: \emph{column\_1 column\_2 ... }. \\
\hline
\end{tabular}
\caption{The list of arguments for the ORDER command.}
\label{table:order_cmd}
\end{center}
\end{table}

\begin{example}
Let's assume to have three input files: \emph{input\_file\_1.txt}, \emph{input\_file\_2.csv} and \emph{input\_file\_3.txt}.
Each file contains the following columns in the order as they are listed: \emph{marker}, \emph{chromosome}, \emph{bp}.
Below are provided commands to rename the column \emph{marker} to \emph{SNPID} and to switch the ordering mode for every input file:
\begin{verbatim}
RENAME   marker SNPID
ORDER    ON chromosome bp SNPID
PROCESS  input_file_1.txt
ORDER    OFF
PROCESS  input_file_2.csv
ORDER    ON
PROCESS  input_file_3.txt
\end{verbatim}
For the input file \emph{input\_file\_1.txt} the columns are re-ordered to: \emph{chromosome}, \emph{bp}, \emph{SNPID}.
For the input file \emph{input\_file\_2.csv} the original column ordering is preserved: \emph{SNPID}, \emph{chromosome}, \emph{bp}.
For the input file \emph{input\_file\_3.txt} the columns are re-ordered following the alphabetical ordering: \emph{bp}, \emph{chromosome}, \emph{SNPID}.
\end{example}

\subsubsection{Filtering}
\emph{gwasformat()} filters SNPs based on minor allele frequency (MAF) and imputation quality.
Table \ref{table:snp_filters} lists the default thresholds.
\begin{table}[!ht]
\begin{center}
\begin{tabular}{|l|p{3cm}|}
\hline
Default column name & Default thresholds \\
\hline
FREQLABEL\index{FREQLABEL} & $> 0.01$ \\
IMP\_QUALITY\index{IMP\_QUALITY} & $> 0.3$ \\
\hline
\end{tabular}
\caption{The default SNP filters.}
\label{table:snp_filters}
\end{center}
\end{table}
The default threshold values can be redefined using the command \textbf{HQ\_SNP}\index{HQ\_SNP} for every input file in the input script.
The command is followed by two values: the first one corresponds to the threshold for the minor allele frequency, and the second one corresponds to the threshold for the imputation quality.

\begin{example}
If we want to filter SNPs with MAF > 0.03 and with imputation quality > 0.4, we would add the following lines to the input script:
\begin{verbatim}
HQ_SNP   0.03 0.4
PROCESS  input_file_1.txt
\end{verbatim}
\end{example}

\begin{example}
If we want to disable filtering, we would change the input script as follows:
\begin{verbatim}
HQ_SNP   0 0
PROCESS  input_file_1.txt
\end{verbatim}
\end{example}

\subsubsection{Inflation Factor and Genomic Control}
By default, \emph{gwasformat()} doesn't calculate the inflation factor and doesn't apply genomic control of P-values and standard errors.
This behaviour can be modified for every input file in the input script using the command \textbf{GC}\index{GC}/\textbf{GENOMICCONTROL}\index{GENOMICCONTROL}.
Table \ref{table:gc_cmd} lists all possible arguments.
\begin{table}[!ht]
\begin{center}
\begin{tabular}{|l|p{9cm}|}
\hline
Argument & Description \\
\hline
OFF & The inflation factor is not calculated and genomic control is not applied (default) \\
ON & The inflation factor is calculated. Genomic control of \emph{PVALUE}\index{PVALUE} and \emph{STDERR}\index{STDERR} columns is applied, corrected value are saved to the new columns \emph{PVALUE\_gc}\index{PVALUE\_gc} and \emph{STDERR\_gc}\index{STDERR\_gc}, accordingly. Has no effect if \emph{PVALUE}\index{PVALUE} column is not present. \\
numeric value & The inflation factor is assumed to be equal to the specified \emph{numeric value}. Values in \emph{PVALUE}\index{PVALUE} and \emph{STDERR}\index{STDERR} columns are corrected and saved to the new columns \emph{PVALUE\_gc}\index{PVALUE\_gc}  and \emph{STDERR\_gc}\index{STDERR\_gc}, accordingly. \\
\hline
\end{tabular}
\caption{The list of arguments for the GC/GENOMICCONTROL command.}
\label{table:gc_cmd}
\end{center}
\end{table}
The inflation factor is calculated using only filtered SNPs.
If calculated or explicitely specified inflation factor is less than $1.0$, then genomic control of P-values and standard errors is not applied, new columns \emph{PVALUE\_gc}\index{PVALUE\_gc} and \emph{STDERR\_gc}\index{STDERR\_gc} are filled with corresponding original values.

\begin{example}
\begin{verbatim}
RENAME	pval pvalue
RENAME	se   stderr
PVALUE	pvalue
STDERR	stderr
GC      ON
PROCESS input_file_1.txt
GC      OFF
PROCESS input_file_2.csv
GC      1.1
PROCESS input_file_3.txt
\end{verbatim}
\end{example}

\subsubsection{Effective Sample Size}
By default, the \emph{gwasformat()} computes the effective sample size based on \emph{IMP\_QUALITY}\index{IMP\_QUALITY} and \emph{N}\index{N} columns.
The computed values are saved to the new column \emph{N\_effective}\index{N\_effective}.

\subsection{GWAS Data Files Annotation}
Often there is a need to annotate markers in GWAS data files with regions (e.g. genes).
The \emph{annotate()} function and its parallel version \emph{pannotate()} allow to quickly annotate every marker in GWAS data files with regions that contain it or fall in a specified windows around it (e.g. +/-50kb, +/-100kb and etc).
The arbitrary number of windows of various sizes can be specified.
The regions with their chromosomal coordinates must be provided in a separate file.
Therefore, it is responsibility of an analyst to prepare the list of regions of interest with chromosomal positions on required human genome build version.
It is possible to annotate markers if only their names are available in GWAS data files, or if there is a need to change chromosomal positions (e.g. if different version of human genome build should be used).
In this case, their chromosomal positions must be provided in a separate map file.

\subsubsection{Input Script}
The annotation instructions are provided to \emph{annotate()} through the input script that is identical to the script required by \emph{gwasqc()}.
Additional commands are supported allowing to specify files with regions for annotation and map files with chromosomal coordinates for markers, if needed.
Example \ref{ex:full_annotation_script} illustrates the content of a hypothetical input script file.

\begin{example} 
\label{ex:full_annotation_script}
\begin{verbatim}
# Description (meaning) of the columns in input files 
# with GWAS data to be annotated.
MARKER     SNPID
CHR		   chr
POSITION   position

# File with regions.
REGIONS_FILE	genes_HGNC_ensembl_rel54_may2009_b36.txt

# Description (meaning) of the columns in file with regions.
REGION_NAME    Gene
REGION_CHR     Chromosome
REGION_START   Start
REGION_END     End

# Window sizes around markers.
REGIONS_DEVIATION   0 10000 25000

# Preserve original columns in the output file and append 
# columns with annotated regions to the end.
REGIONS_APPEND   ON

# Specify prefix for output files with annotation results.
PREFIX   annotated_

# Input file with GWAS data to annotate.
PROCESS   gwa_data_example_1.txt

# Next file with GWAS data will be annotated using marker 
# chromosomal positions in the external map file.
MAP_FILE   map_b37.txt

# Description (meaning) of the columns in map file.
MAP_MARKER     snpid
MAP_CHR        chr_b37
MAP_POSITION   pos_b37

# Change the regions file.
REGIONS_FILE   genes_HGNC_ensembl_rel67_may2012_b37.txt

# Change window sizes around markers.
REGIONS_DEVIATION   0 25000 50000

# Output only columns with annotated regions, marker 
# names and their chromosomal positions.
REGIONS_APPEND   OFF

# Another input file with GWAS data to annotate.
PROCESS   gwa_data_example_2.tbl 
\end{verbatim}
\end{example}

\subsubsection{Specifying The Input Data Files}
The names of the GWAS data files are specified in the input script with the command \textbf{PROCESS}\index{PROCESS} (one line per file). 
A different directory path can be specified for each file.
	
\begin{example}
\begin{verbatim}
PROCESS		input_file_1.txt\cr
PROCESS		/dir_1/dir_2/input_file_2.csv	
\end{verbatim}
The annotation is applied first to \emph{input\_file\_1.txt} and then to \emph{input\_file\_2.csv}.
\end{example}

\subsubsection{Specifying The Regions Files}
The names of the regions (e.g. with genes) files are specified in the input script with the command \textbf{REGIONS\_FILE}\index{REGIONS\_FILE}.
In the same script different regions files can be specified for different GWAS data files.
Also different directory path can be specified for each regions file.

\begin{example}
\begin{verbatim}
REGIONS_FILE	genes_file_1.txt\cr
PROCESS			input_file_1.txt\cr
REGIONS_FILE	/dir_1/dir_2/genes_file_2.csv\cr
PROCESS			input_file_2.csv\cr
PROCESS			input_file_3.txt
\end{verbatim}
The annotation is applied first to \emph{input\_file\_1.txt} using regions from \emph{genes\_file\_1.txt} file.
Then, files \emph{input\_file\_2.csv} and \emph{input\_file\_3.txt} are annotated with regions in \emph{genes\_file\_2.csv} file.
\end{example}

\subsubsection{Specifying The Map Files}
The names of the map files are specified in the input script with the command \textbf{MAP\_FILE}\index{MAP\_FILE}.
In the same script different map files can be specified for different GWAS data files.
Also different directory path can be specified for each map files.
	
\begin{example}
\begin{verbatim}
MAP_FILE		map_file_1.txt
REGIONS_FILE	genes_file_1.txt
PROCESS			input_file_1.txt
MAP_FILE		/dir_1/dir_2/map_file_2.csv
REGIONS_FILE	/dir_1/dir_2/genes_file_2.csv
PROCESS			input_file_2.csv
PROCESS			input_file_3.txt
\end{verbatim}
The annotation is applied first to \emph{input\_file\_1.txt} using marker genomic positions in \emph{map\_file\_1.txt} file and regions in \emph{genes\_file\_1.txt} file.
Then, files \emph{input\_file\_2.csv} and \emph{input\_file\_3.txt} are annotated with regions in \emph{genes\_file\_2.csv} file using marker genomic positions in \emph{map\_file\_2.csv}.
\end{example}

\subsubsection{Specifying Column Names in Input Data Files}
In table \ref{table:default_columns_annotation} the complete list of default column names that must be present in the GWAS data files for annotation is reported. 
These names identify uniquely the items in the GWAS data file for annotation purposes.
\begin{table}[!ht]
\begin{center}
\begin{tabular}{|l|p{8cm}|}
\hline
Default column name(s) & Description \\
\hline
MARKER\index{MARKER} & Marker name \\
CHR\index{CHR} & Chromosome number or name \\
POSITION\index{POSITION} & Marker position \\
\hline
\end{tabular}
\caption{The default column names for annotation.}
\label{table:default_columns_annotation}
\end{center}
\end{table}

Given that different names can be provided for each GWAS data file, \emph{annotate()} allows to redefine the default values for every input file in the input script. 
The redefinition command consists of the default column name followed by the new column name.
When the map file is specified using command \textbf{MAP\_FILE}\index{MAP\_FILE}, then \emph{CHR} and \emph{POSITION} columns in the GWAS data file are not required.
	
\begin{example}
Let's assume to have two input files, \emph{input\_file\_1.txt} and \emph{input\_file\_2.csv}. 
In the \emph{input\_file\_1.txt}, the column names for marker name, chromosome name and position are \emph{SNPID}, \emph{CHR} and \emph{POS}, respectively. 
In the \emph{input\_file\_2.csv}, the column names for marker name is the same as in \emph{input\_file\_1.txt}, but the column names for the chromosome and position are \emph{chromosome} and \emph{position}, respectively. 
The correct column redefinition is as follows:
\begin{verbatim}
MARKER		SNPID
POSITION	POS
PROCESS		input_file_1.txt
CHR			chromosome
POSITION	position
PROCESS		input_file_2.csv
\end{verbatim}
There are no need to define the \emph{CHR} field for the \emph{input\_file\_1.txt}, since it matches the default name.
\end{example}

\subsubsection{Specifying Column Names in Regions Files}
In table \ref{table:default_columns_regions_file} the complete list of default column names for the regions file is reported. 
These names identify uniquely the items in the regions file.
\begin{table}[!ht]
\begin{center}
\begin{tabular}{|l|p{8cm}|}
\hline
Default column name(s) & Description \\
\hline
REGION\_NAME\index{REGION\_NAME} & Region name (e.g. gene name) \\
REGION\_CHR\index{REGION\_CHR} & Chromosome number or name \\
REGION\_START\index{REGION\_START} & Region (e.g. gene) start position \\
REGION\_END\index{REGION\_END} & Region (e.g.) end position \\
\hline
\end{tabular}
\caption{The default column names in regions file.}
\label{table:default_columns_regions_file}
\end{center}
\end{table}

Given that different names can be provided for each regions file, \emph{annotate()} allows to redefine the default values for every regions file in the input script. 
The redefinition command consists of the default column name followed by the present column name. 

\begin{example}	
Let's assume to have two map files, \emph{region\_file\_1.txt} and \emph{region\_file\_2.csv}. 
In the \emph{region\_file\_1.txt}, the column names for the region name, chromosome, start and end position are \emph{name}, \emph{chr}, \emph{REGION\_START} and \emph{REGION\_END}, respectively. 
In the \emph{region\_file\_2.csv}, the column name for the region name and chromosome are the same as in \emph{regions\_file\_1.txt}, but the column names for the region start and end positions are \emph{start} and \emph{end}, respectively. 
The correct column redefinition is as follows:
\begin{verbatim}
REGIONS_FILE	genes_file_1.txt
REGION_NAME		name
REGION_CHR		chr
PROCESS			input_file_1.txt
REGIONS_FILE	genes_file_2.csv
REGION_START 	start
REGION_END		end
PROCESS			input_file_2.csv
\end{verbatim}
There is no need to define the \emph{REGION\_START} and \emph{REGION\_END} fields for \emph{genes\_file\_1.txt} regions file.
Also there is no need to redefine \emph{REGION\_NAME} and \emph{REGION\_CHR} fields for the \emph{genes\_file\_2.csv} map file.	
\end{example}

\subsubsection{Specifying Column Names in Map Files}
In table \ref{table:default_columns_map_file} the complete list of default column names for the map file is reported. 
These names identify uniquely the items in the map file.
\begin{table}[!ht]
\begin{center}
\begin{tabular}{|l|p{8cm}|}
\hline
Default column name(s) & Description \\
\hline
MAP\_MARKER\index{MAP\_MARKER} & Marker name \\
MAP\_CHR\index{MAP\_CHR} & Chromosome number or name \\
MAP\_POSITION\index{MAP\_POSITION} & Marker position \\
\hline
\end{tabular}
\caption{The default column names in map file.}
\label{table:default_columns_map_file}
\end{center}
\end{table}

Given that different names can be provided for each map file, \emph{annotate()} allows to redefine the default values for every map file in the input script. 
The redefinition command consists of the default column name followed by the present column name. 
	
\begin{example}
Let's assume to have two map files, \emph{map\_file\_1.txt} and \emph{map\_file\_2.csv}. 
In the \emph{map\_file\_1.txt}, the column names for marker name, chromosome and position are \emph{name}, \emph{MAP\_CHR} and \emph{pos}, respectively. 
In the \emph{map\_file\_2.csv}, the column name for the marker name and chromosome are the same as in \emph{map\_file\_1.txt}, but the column name for the marker position is \emph{map\_pos}. 
The correct column redefinition is as follows:
\begin{verbatim}
MAP_FILE		map_file_1.txt
MAP_MARKER		name
MAP_POSITION	pos
REGIONS_FILE	genes_file_1.txt
PROCESS			input_file_1.txt
MAP_FILE		map_file_2.csv
MAP_POSITION	map_pos
REGIONS_FILE	genes_file_2.csv
PROCESS			input_file_2.csv
\end{verbatim}
There is no need to define the \emph{MAP\_CHR} field for both map files.
Also there is no need to redefine \emph{MAP\_MARKER} for the \emph{genes\_file\_2.csv} map file.	
\end{example}

\subsubsection{Specifying Window Size For Annotation}
Every marker in the GWAS data file is annotated with the regions (e.g. genes) that fall in a particular window around it.
\emph{annotate()} allows to specify multiple window sizes using command \textbf{REGIONS\_DEVIATION}\index{REGIONS\_DEVIATION}.
Command \textbf{REGIONS\_DEVIATION}\index{REGIONS\_DEVIATION} is followed by an arbitrary number of positive integers that specify window sizes around markers in base pairs.
Each specified window size results in a new output column where all regions overlapping with this window are reported.
The ouptut columns are ordered by window size starting with the smallest.
Therefore, every new output column represents bigger window size and lists only those regions that were not reported previously.
If \textbf{REGIONS\_DEVIATION}\index{REGIONS\_DEVIATION} is not specified, then the default window sizes are 0, 100000 and 250000 (i.e. 0, +/-100kb and +/- 250kb around marker).
If 0 is specified, then only regions that include the marker are reported.
	
\begin{example}
\begin{verbatim}
REGIONS_FILE		genes_file_1.txt
REGIONS_DEVIATION	0 50000 100000
PROCESS				input_file_1.txt
REGIONS_DEVIATION	0 100000 250000 500000
PROCESS				input_file_2.csv
\end{verbatim}
Every marker in \emph{input\_file\_1.txt} will be annotated with regions that contains it or are within +/-50kb and +/-100kb windows around it.
While every marker in \emph{input\_file\_2.csv} will be annotated with regions that contains it or are within +/-100kb, +/-250kb and +/-500kb windows around it.
\end{example}

\subsubsection{Specifying Output Format}
Often GWAS data file contains many columns that are not required in the output files with annotation results.
By default, in addition to columns with annotated regions, \emph{annotate()} outputs only columns with marker name, chromosome name and position.
This behaviour can be modified for every input file in the input script using the command \textbf{REGIONS\_APPEND}\index{REGIONS\_APPEND}.
Table \ref{table:regions_append_cmd} lists all possible arguments.
\begin{table}[!ht]
\begin{center}
\begin{tabular}{|p{4cm}|p{7.5cm}|}
\hline
Argument & Description \\
\hline
OFF & Only the original columns with marker name, chromosome name and position are preserved. Columns with annotated regions are appended to the end. (default) \\
ON & All the original columns are preserved and columns with annotated regions are appended to the end.\\
\hline
\end{tabular}
\caption{The list of arguments for the REGIONS\_APPEND command.}
\label{table:regions_append_cmd}
\end{center}
\end{table}

\begin{example}
\begin{verbatim}
REGIONS_FILE		genes_file_1.txt
REGIONS_APPEND		ON
PROCESS				input_file_1.txt
REGIONS_APPEND		OFF
PROCESS				input_file_2.csv
\end{verbatim}
\end{example}

\begin{thebibliography}{9}

\bibitem{Willer10}
Cristen J. Willer, Yun Li, and Gon\c{c}alo R. Abecasis. (2010) \textbf{METAL: fast and efficient meta-analysis of genomewide association scans}. Bioinformatics 26: 2190-2191.

\bibitem{Bakker08}
Paul I.W. de Bakker, Manuel A.R. Ferreira, Xiaoming Jia, Benjamin M. Neale, Soumya Raychaudhuri, and Benjamin F. Voight (2008) \textbf{Practical aspects of imputation-driven meta-analysis of genome-wide association studies}. Hum. Mol. Genet. 17: R122-R128.

\bibitem{Cramer46}
H. Cramer (1946) \textbf{Mathematical Methods of Statistics}. Princeton: Princeton University Press.

\bibitem{Fisher30}
R.A. Fisher (1930) \textbf{The moments of the distribution for normal samples of measures of departure from normality}. Proc. R. Soc. Series A 130:16-28.

\bibitem{Joanes98}
D.N. Joanes, C.A. Gill (1998) \textbf{Comparing Measures of Sample Skewness and Kurtosis}. J. Royal Stat. Soc. Series D (The Statistician) 47(1):183-189.

\end{thebibliography}

\clearpage
\phantomsection
\addcontentsline{toc}{section}{Index}
\printindex

\end{document}
